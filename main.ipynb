{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:/Projects/TFM/dataset/AD_MCI_HC_WINDOWED\"\n",
    "INDEX_PATH = \"C:/Projects/TFM/dataset/AD_MCI_HC_WINDOWED/data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entities.graphs.graph_builder import RawAndPearson, MomentsAndPearson, MomentsAndPLI, RawAndPLI, PSDAndCSD, PSDAndPearson, OfflineGeneric\n",
    "from entities.graphs.data_reader import read_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, indices, builder, transform=None, target_transform=None):\n",
    "        self.indices = indices\n",
    "        self.builder = builder\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        current_path = self.indices.iloc[idx][\"path\"]\n",
    "        raw_data = read_record(current_path)\n",
    "        label = self.indices.iloc[idx][\"label\"]\n",
    "        data = self.builder.build(raw_data, label)\n",
    "        return data\n",
    "    \n",
    "class OfflineDataset(Dataset):\n",
    "    def __init__(self, node_indices, edge_indices, builder, transform=None, target_transform=None):\n",
    "        self.node_indices = node_indices\n",
    "        self.edge_indices = edge_indices\n",
    "        self.builder = builder\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.node_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        current_path_nodes = self.node_indices.iloc[idx][\"path\"]\n",
    "        computed_nodes = read_record(current_path_nodes)\n",
    "        \n",
    "        current_path_edges = self.edge_indices.iloc[idx][\"path\"]\n",
    "        computed_edges = read_record(current_path_edges)\n",
    "        \n",
    "        label = self.node_indices.iloc[idx][\"label\"]\n",
    "        data = self.builder.build(computed_nodes, computed_edges, label)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MODE = \"OFFLINE\"\n",
    "\n",
    "if MODE == \"OFFLINE\":\n",
    "\n",
    "    NODE_INDEX_PATH = \"C:/Projects/TFM/dataset/AD_MCI_HC_PSD/data.csv\"\n",
    "    EDGE_INDEX_PATH = \"C:/Projects/TFM/dataset/AD_MCI_HC_PEARSON/data.csv\"\n",
    "\n",
    "    node_indices = pd.read_csv(NODE_INDEX_PATH, index_col=\"Unnamed: 0\")\n",
    "    edge_indices = pd.read_csv(EDGE_INDEX_PATH, index_col=\"Unnamed: 0\")\n",
    "\n",
    "    node_indices = node_indices.drop(node_indices[node_indices.label == \"MCI\"].index)\n",
    "    node_indices_hc = node_indices[node_indices.label == 'HC'].sample(frac=0.4)\n",
    "    node_indices_ad = node_indices[node_indices.label == 'AD']\n",
    "    node_indices = pd.concat([node_indices_hc, node_indices_ad])\n",
    "\n",
    "    node_train_indices, node_test_indices = train_test_split(node_indices, shuffle=True)\n",
    "    edge_train_indices, edge_test_indices = edge_indices.iloc[node_train_indices.index], edge_indices.iloc[node_test_indices.index]\n",
    "\n",
    "    builder = OfflineGeneric(th=None)\n",
    "    \n",
    "    train_dataset = OfflineDataset(node_train_indices, edge_train_indices, builder)\n",
    "    test_dataset = OfflineDataset(node_test_indices, edge_test_indices, builder)\n",
    "    \n",
    "if MODE == \"ONLINE\":\n",
    "    indices = pd.read_csv(INDEX_PATH, index_col=\"Unnamed: 0\")\n",
    "    indices = indices.drop(indices[indices.label == \"MCI\"].index)\n",
    "    indices_hc = indices[indices.label == 'HC'].sample(frac=0.4)\n",
    "    indices_ad = indices[indices.label == 'AD']\n",
    "    indices = pd.concat([indices_hc, indices_ad])\n",
    "\n",
    "    train_data, test_data = train_test_split(indices, shuffle=True)\n",
    "\n",
    "    builder = RawAndPearson(normalize_nodes=True, normalize_edges=False, th=0)\n",
    "    #builder = MomentsAndPearson(th=0)\n",
    "    #builder = MomentsAndPLI()\n",
    "    #builder = RawAndPLI(normalize_nodes=True, normalize_edges=False)\n",
    "    #builder = PSDAndCSD()\n",
    "    #builder = PSDAndPearson(th=0.5)\n",
    "\n",
    "    train_dataset = BaseDataset(train_data, builder)\n",
    "    test_dataset = BaseDataset(test_data, builder)\n",
    "    train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "_BATCH_SIZE = 128\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=_BATCH_SIZE, shuffle=True)#sampler=weighted_sampler)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=_BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.8407, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.8407, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.7872],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.7628],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.7872, 0.7628, 0.0000]],\n",
      "       dtype=torch.float64) torch.Size([2432, 6])\n",
      "torch.Size([19, 6])\n"
     ]
    }
   ],
   "source": [
    "def check_data():\n",
    "    for data in train_dataloader:\n",
    "        print(data.edge_attr, data.x.shape)\n",
    "        print(next(iter(data[0]))[1].shape)\n",
    "        break\n",
    "\n",
    "check_data()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entities.models.factory import ModelFactory\n",
    "from entities.models.modelsTypes import Model\n",
    "\n",
    "model_factory = ModelFactory()\n",
    "model = model_factory.create(Model.EEGCONVNETMINIV2)\n",
    "model = model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EEGConvNetMiniV2(\n",
       "  (conv1): GCNConv(-1, 32)\n",
       "  (conv2): GCNConv(32, 64)\n",
       "  (batch_norm1): BatchNorm(32)\n",
       "  (batch_norm2): BatchNorm(64)\n",
       "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (fc3): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()#weight=torch.tensor([3, 1], dtype=torch.float64)) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True, patience=5, threshold=0.05, cooldown=2, factor=0.5, min_lr=1e-5)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    30] loss: 1.002\n",
      "Training epoch 0: 0.980 loss\n",
      "Train Bal.ACC: 0.625, test Bal.ACC: 0.621\n",
      "Epoch: 000, Train Acc: 0.6160, Test Acc: 0.6202\n",
      "[2,    30] loss: 0.614\n",
      "Training epoch 1: 0.611 loss\n",
      "Train Bal.ACC: 0.649, test Bal.ACC: 0.656\n",
      "Epoch: 001, Train Acc: 0.6382, Test Acc: 0.6547\n",
      "[3,    30] loss: 0.558\n",
      "Training epoch 2: 0.561 loss\n",
      "Train Bal.ACC: 0.722, test Bal.ACC: 0.717\n",
      "Epoch: 002, Train Acc: 0.7238, Test Acc: 0.7176\n",
      "[4,    30] loss: 0.548\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, balanced_accuracy_score\n",
    "from numpy import mean\n",
    "import numpy as np\n",
    "\n",
    "auroc_train_history = []\n",
    "auroc_test_history = []\n",
    "balACC_train_history = []\n",
    "\n",
    "balACC_test_history = []\n",
    "loss_train_history = []\n",
    "loss_test_history = []\n",
    "\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "_NUM_EPOCHS = 120\n",
    "_DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    batch_loss = []\n",
    "    mean_batch_loss = 0\n",
    "    for i, data in enumerate(train_dataloader):  # Iterate in batches over the training dataset.\n",
    "\n",
    "        #data.batch = data.batch.view(data.batch.shape[0], -1)\n",
    "        #print(data.x[0].shape)\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        out = model(data.x, data.edge_index, data.edge_attr,\n",
    "                    data.batch)  # Perform a single forward pass.\n",
    "        \n",
    "        \n",
    "\n",
    "        loss = criterion(out, data.label)  # Compute the loss.\n",
    "        #print(loss.item())\n",
    "        batch_loss.append(loss.item())\n",
    "        #print(data.label)\n",
    "        loss.backward()  # Derive gradients.\n",
    "          # Update parameters based on gradients.\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i%30 == 29:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 30:.3f}')\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    mean_batch_loss = mean(batch_loss)\n",
    "    print(f\"Training epoch {epoch}: {mean_batch_loss:.3f} loss\")\n",
    "    scheduler.step(mean_batch_loss)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    y_probs_train = torch.empty(0, 2).to(_DEVICE)\n",
    "    y_true_train, y_pred_train = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "            out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "            y_batch = data.label.to(device=_DEVICE, non_blocking=True)\n",
    "            \n",
    "            pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            \n",
    "            #print(pred)\n",
    "            \n",
    "            correct += int((pred == data.label).sum())  # Check against ground-truth labels.\n",
    "            y_pred_train += pred.cpu().numpy().tolist()\n",
    "            \n",
    "            y_probs_train = torch.cat((y_probs_train, out.data), 0)\n",
    "            y_true_train += y_batch.cpu().numpy().tolist()\n",
    "            \n",
    "    y_probs_train = torch.nn.functional.softmax(y_probs_train, dim=1).cpu().numpy()\n",
    "    y_true_train = np.array(y_true_train)\n",
    "\n",
    "    return correct / len(loader.dataset), y_true_train, y_probs_train, y_pred_train  # Derive ratio of correct predictions.\n",
    "\n",
    "best_model_test_acc = 0\n",
    "for epoch in range(300):\n",
    "    train()\n",
    "    \n",
    "    train_acc, y_true_train, y_probs_train, y_pred_train = test(train_dataloader)\n",
    "    test_acc, y_true_test, y_probs_test, y_pred_test = test(test_dataloader)\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    \n",
    "    balACC_train_history.append(balanced_accuracy_score(y_true_train, y_pred_train))\n",
    "    balACC_test_history.append(balanced_accuracy_score(y_true_test, y_pred_test))\n",
    "\n",
    "    print(f\"Train Bal.ACC: {balACC_train_history[-1]:.3f}, test Bal.ACC: {balACC_test_history[-1]:.3f}\")\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    \n",
    "    if test_acc > best_model_test_acc: torch.save(model.state_dict(), './trained_models/eegconvnetminiv2-128bs-PSDAndPearson-noTH.pt')\n",
    "    #scheduler.step(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_true_test, y_pred_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=[\"AD\", \"HC\"])\n",
    "\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:/Projects/TFM/dataset/AD_MCI_HC_WINDOWED\"\n",
    "INDEX_PATH = \"C:/Projects/TFM/dataset/AD_MCI_HC_WINDOWED/data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from entities.graphs.data_reader import read_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(INDEX_PATH)\n",
    "dataPath = df.iloc[828].path\n",
    "dataPath2 = df.iloc[20].path\n",
    "record = read_record(dataPath)\n",
    "record2 = read_record(dataPath2)\n",
    "#df[df[\"path\"].str.contains(\"Patient_2_AD_T0_filtered_0\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entities.graphs.node_extractors import PSDExtractor\n",
    "from entities.graphs.edge_extractors import SpectralCoherenceExtractor\n",
    "from scipy.signal import csd\n",
    "\n",
    "e = SpectralCoherenceExtractor()\n",
    "xd = e.extract_features(record2)\n",
    "xd.shape\n",
    "\n",
    "#a = [[np.abs(np.mean(csd(ch1, ch2, fs=256)[1])) for ch2 in record] for ch1 in record]\n",
    "#a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying PLI implementation using Hilbert Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import hilbert\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "hilbert_transform = np.imag(hilbert(record, axis=1))\n",
    "phase_diff = np.zeros((hilbert_transform.shape[0], hilbert_transform.shape[0]))\n",
    "\n",
    "for row in range(19):\n",
    "    for col in range(19):\n",
    "        phase_diff[row, col] = np.abs(np.mean(np.exp(1j*(hilbert_transform[row] - hilbert_transform[col]))))\n",
    "\n",
    "phase_diff\n",
    "#hilbert_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data, normalize=False, th=None) -> None:\n",
    "\n",
    "        rows, cols = data.shape\n",
    "        PLImatrix = np.zeros((rows, rows))    \n",
    "        \n",
    "        hilbert_transform = np.imag(hilbert(data))\n",
    "        \n",
    "        if normalize:\n",
    "            data = normalize(data)\n",
    "            \n",
    "        for row in range(rows):\n",
    "            for col in range(rows):\n",
    "                PLImatrix[row, col] = np.abs(np.mean(np.exp(1j * (hilbert_transform[row] - hilbert_transform[col]))))\n",
    "          \n",
    "        # Set to 0 values undes a th\n",
    "        if th:\n",
    "            PLImatrix = np.where(\n",
    "                abs(PLImatrix) < th, 0, PLImatrix\n",
    "            )\n",
    "        \n",
    "        # Remove self loops\n",
    "        PLImatrix = np.where(\n",
    "                abs(PLImatrix) == 1.0, 0, PLImatrix\n",
    "            )\n",
    "        \n",
    "        PLImatrix = torch.from_numpy(PLImatrix)\n",
    "        return PLImatrix\n",
    "\n",
    "extract_features(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empt = np.ndarray((2, 19, 1280))\n",
    "empt[0] = record\n",
    "empt[1] = record2\n",
    "a = spectral_connectivity_epochs(empt, method=\"pli\", sfreq=256, mode=\"multitaper\")\n",
    "data = a.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import csd\n",
    "\n",
    "def expectation(data):\n",
    "    vals = np.unique(data)\n",
    "    probs = [\n",
    "        np.count_nonzero(data == val) / len(data) for val in vals\n",
    "    ]\n",
    "    return np.sum(vals * probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, sxy = csd(record[0], record[2], fs=256)\n",
    "expectation(np.sign(np.imag(sxy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_pli(x, y):\n",
    "    _, cpsd = csd(x, y, fs=256)\n",
    "    phase_lag_index = np.mean(np.sign(np.imag(cpsd[30:60])))\n",
    "    return phase_lag_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows, cols = record.shape\n",
    "PLImatrix = np.zeros((rows, rows))    \n",
    "    \n",
    "    \n",
    "for row in range(rows):\n",
    "    for col in range(rows):\n",
    "        PLImatrix[row, col] = _compute_pli(record[row], record[col])\n",
    "\n",
    "\n",
    "#PLImatrix = torch.from_numpy(PLImatrix)\n",
    "PLImatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spectral_connectivity import Multitaper, Connectivity\n",
    "\n",
    "# Compute multitaper spectral estimate\n",
    "signal = np.zeros((1280, 19))\n",
    "signal = np.reshape(record, (1280, 1, 19))\n",
    "m = Multitaper(time_series=signal,\n",
    "               sampling_frequency=256,\n",
    "               start_time=0)\n",
    "\n",
    "# Sets up computing connectivity measures/power from multitaper spectral estimate\n",
    "c = Connectivity.from_multitaper(m)\n",
    "\n",
    "# Here are a couple of examples\n",
    "power = c.power() # spectral power\n",
    "coherence = c.coherence_magnitude()\n",
    "phase_lag_index = c.phase_lag_index()\n",
    "#canonical_coherence = c.canonical_coherence(brain_area_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(c.frequencies).index(10.4))\n",
    "\n",
    "phase_lag_index[0][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bedda92429613c38dce8c018b5f564564fc8e6e3bd92ce59184d55779d13ad8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('tfm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
