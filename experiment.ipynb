{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from entities.graphs.data_reader import read_record\n",
    "from entities.graphs.graph_builder import RawAndPearson, MomentsAndPearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:/Projects/TFM/dataset/AD_MCI_HC_WINDOWED\"\n",
    "INDEX_PATH = \"C:/Projects/TFM/dataset/AD_MCI_HC_WINDOWED/data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"batch_size\": 64,\n",
    "    \"lr\": 0.1,\n",
    "    \"momentum\": 0.9,\n",
    "    \"epochs\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Removing MCI patients to retrieve only HC and AD subjects for binary classification\n",
    "indices = pd.read_csv(INDEX_PATH, index_col=\"Unnamed: 0\")\n",
    "indices = indices.drop(indices[indices.label == \"MCI\"].index)\n",
    "indices_hc = indices[indices.label == 'HC'].sample(frac=0.4)\n",
    "indices_ad = indices[indices.label == 'AD']\n",
    "indices = pd.concat([indices_hc, indices_ad])\n",
    "\n",
    "train_data, test_data = train_test_split(indices, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, indices, builder, transform=None):\n",
    "        self.index_df = indices\n",
    "        self.transform = transform\n",
    "        self.builder = builder\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.index_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        current_path = self.index_df.iloc[idx][\"path\"]\n",
    "        raw_data = read_record(current_path)\n",
    "        label = self.index_df.iloc[idx][\"label\"]\n",
    "        data = self.builder.build(raw_data, label)\n",
    "        sample = {\n",
    "            \"x\": data.x,\n",
    "            \"edge_attr\": data.edge_attr,\n",
    "            \"edge_index\": data.edge_index,\n",
    "            \"label\": data.label\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()]\n",
    ") \n",
    "# Split using tensorflow: Shuffling not that good, more useful when using three classes instead of 2\n",
    "\"\"\"eeg_dataset = EEGDataset(INDEX_PATH, PATH, RawAndPearson(normalize_nodes=True, normalize_edges=True), transform=transform)\n",
    "train_size = math.floor(len(eeg_dataset) * 0.7)\n",
    "test_szie = len(eeg_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(eeg_dataset, [train_size, test_szie])\"\"\"\n",
    "#builder = RawAndPearson(normalize_nodes=True, normalize_edges=True)\n",
    "builder = MomentsAndPearson()\n",
    "train_dataset = EEGDataset(train_data, builder, transform=transform)\n",
    "test_dataset = EEGDataset(test_data, builder, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element 0: Nodes: torch.Size([19, 6]), Edge attributes: torch.Size([19, 19]), Label: 0\n",
      "Element 1: Nodes: torch.Size([19, 6]), Edge attributes: torch.Size([19, 19]), Label: 0\n",
      "Element 2: Nodes: torch.Size([19, 6]), Edge attributes: torch.Size([19, 19]), Label: 0\n",
      "Element 3: Nodes: torch.Size([19, 6]), Edge attributes: torch.Size([19, 19]), Label: 0\n",
      "Element 4: Nodes: torch.Size([19, 6]), Edge attributes: torch.Size([19, 19]), Label: 1\n",
      "Element 5: Nodes: torch.Size([19, 6]), Edge attributes: torch.Size([19, 19]), Label: 0\n",
      "Element 6: Nodes: torch.Size([19, 6]), Edge attributes: torch.Size([19, 19]), Label: 1\n",
      "Element 7: Nodes: torch.Size([19, 6]), Edge attributes: torch.Size([19, 19]), Label: 0\n",
      "Element 8: Nodes: torch.Size([19, 6]), Edge attributes: torch.Size([19, 19]), Label: 0\n",
      "Element 9: Nodes: torch.Size([19, 6]), Edge attributes: torch.Size([19, 19]), Label: 1\n",
      "Element 10: Nodes: torch.Size([19, 6]), Edge attributes: torch.Size([19, 19]), Label: 1\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(train_dataset):\n",
    "    print(f\"Element {i}: Nodes: {sample['x'].size()}, Edge attributes: {sample['edge_attr'].size()}, Label: {sample['label']}\")\n",
    "    if i == 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=hyperparams[\"batch_size\"],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataset = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=hyperparams[\"batch_size\"],\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([64, 19, 6]) torch.Size([64, 19, 19]) torch.Size([64])\n",
      "1 torch.Size([64, 19, 6]) torch.Size([64, 19, 19]) torch.Size([64])\n",
      "2 torch.Size([64, 19, 6]) torch.Size([64, 19, 19]) torch.Size([64])\n",
      "3 torch.Size([64, 19, 6]) torch.Size([64, 19, 19]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for i_batch, sample_batched in enumerate(train_dataset):\n",
    "    print(i_batch, sample_batched[\"x\"].size(), sample_batched[\"edge_attr\"].size(), sample_batched[\"label\"].size())\n",
    "    if i_batch == 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network model: \n",
    "class EEGSmall(nn.Module):\n",
    "\n",
    "    def __init__(self, batch_size=8, **kwargs):\n",
    "        super(EEGSmall, self).__init__()\n",
    "       \n",
    "        #self.batch_norm = BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        input_size = kwargs.get(\"input_size\", -1)\n",
    "        output_size = kwargs.get(\"output_size\", 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_size)\n",
    "        \n",
    "        # Xavier initializacion for fully connected layers\n",
    "        self.fc1.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc2.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        self.fc3.apply(lambda x: nn.init.xavier_normal_(x.weight, gain=1) if isinstance(x, nn.Linear) else None)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weigth, batch):\n",
    "       \n",
    "        # Perform batch normalization\n",
    "        #x = F.leaky_relu(self.batch_norm(x), negative_slope=0.01)\n",
    "        #x = F.dropout(batch_norm_out, p=0.2, training=self.training)\n",
    "        # Global add pooling\n",
    "        #mean_pool = global_add_pool(x, batch=batch)\n",
    "        \n",
    "        #print(\"Original nodes size: \", x.size())\n",
    "        #print(batch.shape)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        #print(\"Nodes size after view: \", x.size())\n",
    "        # Apply fully connected layters\n",
    "        out = F.leaky_relu(self.fc1(x), negative_slope=0.01)\n",
    "        #out = F.dropout(out, p = 0.2, training=self.training)\n",
    "        \n",
    "        out = F.leaky_relu(self.fc2(out), negative_slope=0.01)\n",
    "        #out = F.dropout(out, p = 0.2, training=self.training)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model: \n",
    "N_CHANNELS = 19\n",
    "N_FEATURES = 6\n",
    "N_CLASSES = 2\n",
    "\n",
    "input_size = N_CHANNELS * N_FEATURES\n",
    "\n",
    "model = EEGSmall(input_size=input_size, output_size=N_CLASSES)\n",
    "model = model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:  9506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EEGSmall(\n",
       "  (fc1): Linear(in_features=114, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(\"Trainable parameters: \", params)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss, optimizer and scheduler (if used)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=hyperparams[\"lr\"], momentum=hyperparams[\"momentum\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', verbose=True, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lokix\\Envs\\tfm\\lib\\site-packages\\torch\\autograd\\__init__.py:173: UserWarning: Error detected in LogSoftmaxBackward0. Traceback of forward call that caused the error:\n",
      "  File \"c:\\users\\lokix\\appdata\\local\\programs\\python\\python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\users\\lokix\\appdata\\local\\programs\\python\\python39\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\lokix\\Envs\\tfm\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\lokix\\Envs\\tfm\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\lokix\\Envs\\tfm\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\lokix\\Envs\\tfm\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\users\\lokix\\appdata\\local\\programs\\python\\python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\users\\lokix\\appdata\\local\\programs\\python\\python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\users\\lokix\\appdata\\local\\programs\\python\\python39\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\lokix\\Envs\\tfm\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 473, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\lokix\\Envs\\tfm\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 462, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\lokix\\Envs\\tfm\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 369, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\lokix\\Envs\\tfm\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 664, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\lokix\\Envs\\tfm\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 355, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"c:\\Users\\lokix\\Envs\\tfm\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\lokix\\Envs\\tfm\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2854, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\lokix\\Envs\\tfm\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2900, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"c:\\Users\\lokix\\Envs\\tfm\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\lokix\\Envs\\tfm\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3098, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\lokix\\Envs\\tfm\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3301, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\lokix\\Envs\\tfm\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3361, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\lokix\\AppData\\Local\\Temp\\ipykernel_21740\\908338394.py\", line 82, in <cell line: 81>\n",
      "    train()\n",
      "  File \"C:\\Users\\lokix\\AppData\\Local\\Temp\\ipykernel_21740\\908338394.py\", line 41, in train\n",
      "    loss = criterion(out, data['label'])  # Compute the loss.\n",
      "  File \"c:\\Users\\lokix\\Envs\\tfm\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\lokix\\Envs\\tfm\\lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1163, in forward\n",
      "    return F.cross_entropy(input, target, weight=self.weight,\n",
      "  File \"c:\\Users\\lokix\\Envs\\tfm\\lib\\site-packages\\torch\\nn\\functional.py\", line 2996, in cross_entropy\n",
      "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
      " (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:104.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'LogSoftmaxBackward0' returned nan values in its 0th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lokix\\OneDrive\\Documents\\univsersidad\\MASTER\\1B\\TFM\\src_v3\\experiment.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 81>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src_v3/experiment.ipynb#ch0000011?line=77'>78</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m correct \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(loader\u001b[39m.\u001b[39mdataset), y_true_train, y_probs_train, y_pred_train  \u001b[39m# Derive ratio of correct predictions.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src_v3/experiment.ipynb#ch0000011?line=80'>81</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(hyperparams[\u001b[39m\"\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src_v3/experiment.ipynb#ch0000011?line=81'>82</a>\u001b[0m     train()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src_v3/experiment.ipynb#ch0000011?line=83'>84</a>\u001b[0m     train_acc, y_true_train, y_probs_train, y_pred_train \u001b[39m=\u001b[39m test(train_dataset)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src_v3/experiment.ipynb#ch0000011?line=84'>85</a>\u001b[0m     test_acc, y_true_test, y_probs_test, y_pred_test \u001b[39m=\u001b[39m test(test_dataset)\n",
      "\u001b[1;32mc:\\Users\\lokix\\OneDrive\\Documents\\univsersidad\\MASTER\\1B\\TFM\\src_v3\\experiment.ipynb Cell 14'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src_v3/experiment.ipynb#ch0000011?line=40'>41</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, data[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m])  \u001b[39m# Compute the loss.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src_v3/experiment.ipynb#ch0000011?line=42'>43</a>\u001b[0m batch_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src_v3/experiment.ipynb#ch0000011?line=43'>44</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# Derive gradients.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src_v3/experiment.ipynb#ch0000011?line=44'>45</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m\"\u001b[39m, loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src_v3/experiment.ipynb#ch0000011?line=45'>46</a>\u001b[0m   \u001b[39m# Update parameters based on gradients.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lokix\\Envs\\tfm\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\lokix\\Envs\\tfm\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Function 'LogSoftmaxBackward0' returned nan values in its 0th output."
     ]
    }
   ],
   "source": [
    "# Nightmare cell where everything goes wrong.\n",
    "# Here I try to train the network but there are only two possible scenarios: \n",
    "\n",
    "#   1. The accuracy is bullshit (very often)\n",
    "#   2. The network throws an error that i do not fucking know how to solve\n",
    "\n",
    "# Have luck!\n",
    "# fails_counter = 5  ----> Add 1 whenever you fail ---- 10/06/2022\n",
    "from sklearn.metrics import roc_auc_score, balanced_accuracy_score\n",
    "from numpy import mean\n",
    "import numpy as np\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "auroc_train_history = []\n",
    "auroc_test_history = []\n",
    "balACC_train_history = []\n",
    "\n",
    "balACC_test_history = []\n",
    "loss_train_history = []\n",
    "loss_test_history = []\n",
    "\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "_NUM_EPOCHS = 120\n",
    "_DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    batch_loss = []\n",
    "    for i, data in enumerate(train_dataset):  # Iterate in batches over the training dataset.\n",
    "\n",
    "        #data.batch = data.batch.view(data.batch.shape[0], -1)\n",
    "        #print(data['x'].shape)\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        out = model(data['x'], data['edge_index'], data['edge_attr'], data)  # Perform a single forward pass.\n",
    "        #print(\"Output shape: \", out.size())\n",
    "        loss = criterion(out, data['label'])  # Compute the loss.\n",
    "        print(\"Prediction: \", out, \"Target: \", data[\"label\"])\n",
    "        batch_loss.append(loss.item())\n",
    "        loss.backward()  # Derive gradients.\n",
    "        print(\"Loss: \", loss.item())\n",
    "          # Update parameters based on gradients.\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i%100 == 99:\n",
    "            print(f'Epoch: {epoch + 1} - Iteration: {i + 1:5d} loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "    print(f\"Training epoch {epoch}: {mean(batch_loss):.3f} loss\")\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    y_probs_train = torch.empty(0, 2).to(_DEVICE)\n",
    "    y_true_train, y_pred_train = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "            out = model(data['x'], data['edge_index'], data['edge_attr'], data)\n",
    "            y_batch = data['label'].to(device=_DEVICE, non_blocking=True)\n",
    "            \n",
    "            pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            \n",
    "            \n",
    "            correct += int((pred == data['label']).sum())  # Check against ground-truth labels.\n",
    "            y_pred_train += pred.cpu().numpy().tolist()\n",
    "            \n",
    "            y_probs_train = torch.cat((y_probs_train, out.data), 0)\n",
    "            y_true_train += y_batch.cpu().numpy().tolist()\n",
    "            \n",
    "    y_probs_train = torch.nn.functional.softmax(y_probs_train, dim=1).cpu().numpy()\n",
    "    y_true_train = np.array(y_true_train)\n",
    "\n",
    "    return correct / len(loader.dataset), y_true_train, y_probs_train, y_pred_train  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(hyperparams[\"epochs\"]):\n",
    "    train()\n",
    "    \n",
    "    train_acc, y_true_train, y_probs_train, y_pred_train = test(train_dataset)\n",
    "    test_acc, y_true_test, y_probs_test, y_pred_test = test(test_dataset)\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    balACC_train_history.append(balanced_accuracy_score(y_true_train, y_pred_train))\n",
    "    balACC_test_history.append(balanced_accuracy_score(y_true_test, y_pred_test))\n",
    "\n",
    "    print(f\"Train Bal.ACC: {balACC_train_history[-1]:.3f}, test Bal.ACC: {balACC_test_history[-1]:.3f}\")\n",
    "    \n",
    "    \n",
    "    print(\n",
    "        f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}'\n",
    "    )\n",
    "    scheduler.step(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 19, 1280])\n",
      "torch.Size([1, 19, 512])\n",
      "Hidden state output LSTM 1 gate: \n",
      "2\n",
      "torch.Size([1, 19, 512])\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([1, 19, 256])\n",
      "Hidden state output LSTM 3 gates: \n",
      "2\n",
      "torch.Size([3, 19, 256])\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([1, 19, 128])\n",
      "torch.Size([1, 19, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from itertools import product\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "lstm = nn.LSTM(1280, 512, 1)\n",
    "lstm2 = nn.LSTM(512, 256, 3) \n",
    "fc1 = nn.Linear(256, 128)\n",
    "conv = GCNConv(128, 64)\n",
    "\n",
    "edge_index = torch.tensor(\n",
    "            [[a, b] for a, b in product(range(19), range(19))]\n",
    "        ).t().contiguous()\n",
    "\n",
    "input = torch.rand(1, 19, 1280)\n",
    "print(input.size())\n",
    "\n",
    "output, hidden = lstm(input)\n",
    "print(output.size())\n",
    "print(\"Hidden state output LSTM 1 gate: \")\n",
    "print(len(hidden))\n",
    "print(hidden[0].size())\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "output, hidden = lstm2(output)\n",
    "print(output.size())\n",
    "print(\"Hidden state output LSTM 3 gates: \")\n",
    "print(len(hidden))\n",
    "print(hidden[0].size())\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "output = fc1(output)\n",
    "print(output.size())\n",
    "\n",
    "output = conv(output, edge_index)\n",
    "print(output.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from autogl.datasets import build_dataset_from_name, utils\n",
    "from autogl.solver import AutoGraphClassifier\n",
    "from autogl.module import Acc\n",
    "from autogl.backend import DependentBackend\n",
    "if DependentBackend.is_pyg():\n",
    "    from autogl.datasets.utils.conversion import to_pyg_dataset as convert_dataset\n",
    "else:\n",
    "    from autogl.datasets.utils.conversion import to_dgl_dataset as convert_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from entities.graphs.graph_builder import RawAndPearson, MomentsAndPearson, MomentsAndPLI, RawAndPLI, PSDAndCSD, PSDAndPearson, OfflineGeneric\n",
    "from entities.graphs.data_reader import read_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, indices, builder, transform=None, target_transform=None):\n",
    "        self.indices = indices\n",
    "        self.builder = builder\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        current_path = self.indices.iloc[idx][\"path\"]\n",
    "        raw_data = read_record(current_path)\n",
    "        label = self.indices.iloc[idx][\"label\"]\n",
    "        data = self.builder.build(raw_data, label)\n",
    "        return data\n",
    "    \n",
    "class OfflineDataset(Dataset):\n",
    "    def __init__(self, node_indices, edge_indices, builder, transform=None, target_transform=None):\n",
    "        self.node_indices = node_indices\n",
    "        self.edge_indices = edge_indices\n",
    "        self.builder = builder\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.node_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        current_path_nodes = self.node_indices.iloc[idx][\"path\"]\n",
    "        computed_nodes = read_record(current_path_nodes)\n",
    "        \n",
    "        current_path_edges = self.edge_indices.iloc[idx][\"path\"]\n",
    "        computed_edges = read_record(current_path_edges)\n",
    "        \n",
    "        label = self.node_indices.iloc[idx][\"label\"]\n",
    "        data = self.builder.build(computed_nodes, computed_edges, label)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "MODE = \"OFFLINE\"\n",
    "\n",
    "if MODE == \"OFFLINE\":\n",
    "\n",
    "    NODE_INDEX_PATH = \"C:/Projects/TFM/dataset/AD_MCI_HC_PSD/data.csv\"\n",
    "    EDGE_INDEX_PATH = \"C:/Projects/TFM/dataset/AD_MCI_HC_PEARSON/data.csv\"\n",
    "\n",
    "    node_indices = pd.read_csv(NODE_INDEX_PATH, index_col=\"Unnamed: 0\")\n",
    "    edge_indices = pd.read_csv(EDGE_INDEX_PATH, index_col=\"Unnamed: 0\")\n",
    "\n",
    "    node_indices = node_indices.drop(node_indices[node_indices.label == \"MCI\"].index)\n",
    "    node_indices_hc = node_indices[node_indices.label == 'HC'].sample(frac=0.4)\n",
    "    node_indices_ad = node_indices[node_indices.label == 'AD']\n",
    "    node_indices = pd.concat([node_indices_hc, node_indices_ad])\n",
    "\n",
    "    node_train_indices, node_test_indices = train_test_split(node_indices, shuffle=True)\n",
    "    edge_train_indices, edge_test_indices = edge_indices.iloc[node_train_indices.index], edge_indices.iloc[node_test_indices.index]\n",
    "\n",
    "    builder = OfflineGeneric(th=None)\n",
    "    \n",
    "    train_dataset = OfflineDataset(node_train_indices, edge_train_indices, builder)\n",
    "    test_dataset = OfflineDataset(node_test_indices, edge_test_indices, builder)\n",
    "    \n",
    "if MODE == \"ONLINE\":\n",
    "    indices = pd.read_csv(INDEX_PATH, index_col=\"Unnamed: 0\")\n",
    "    indices = indices.drop(indices[indices.label == \"MCI\"].index)\n",
    "    indices_hc = indices[indices.label == 'HC'].sample(frac=0.4)\n",
    "    indices_ad = indices[indices.label == 'AD']\n",
    "    indices = pd.concat([indices_hc, indices_ad])\n",
    "\n",
    "    train_data, test_data = train_test_split(indices, shuffle=True)\n",
    "\n",
    "    builder = RawAndPearson(normalize_nodes=True, normalize_edges=False, th=0)\n",
    "    #builder = MomentsAndPearson(th=0)\n",
    "    #builder = MomentsAndPLI()\n",
    "    #builder = RawAndPLI(normalize_nodes=True, normalize_edges=False)\n",
    "    #builder = PSDAndCSD()\n",
    "    #builder = PSDAndPearson(th=0.5)\n",
    "\n",
    "    train_dataset = BaseDataset(train_data, builder)\n",
    "    test_dataset = BaseDataset(test_data, builder)\n",
    "    train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogl.module.model.pyg import AutoGIN\n",
    "# from autogl.module.model.dgl import AutoGIN  # dgl version\n",
    "model = AutoGIN(\n",
    "                num_features=6,\n",
    "                num_classes=2,\n",
    "                num_graph_features=0,\n",
    "                init=False\n",
    "            ).from_hyper_parameter({\n",
    "                # hp from model\n",
    "                \"num_layers\": 5,\n",
    "                \"hidden\": [64,64,64,64],\n",
    "                \"dropout\": 0.5,\n",
    "                \"act\": \"relu\",\n",
    "                \"eps\": \"False\",\n",
    "                \"mlp_layers\": 2,\n",
    "                \"neighbor_pooling_type\": \"sum\",\n",
    "                \"graph_pooling_type\": \"sum\"\n",
    "            }).model\n",
    "            \n",
    "model = model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "_BATCH_SIZE = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=_BATCH_SIZE, shuffle=True)#sampler=weighted_sampler)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=_BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the loss optimizer.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "device = torch.device('cpu')\n",
    "# Training\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    for data in train_dataloader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, data.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5198501872659176"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        correct += pred.eq(data.label).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "acc = test(model, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = AutoGraphClassifier(\n",
    "            feature_module=None,\n",
    "            graph_models=[],\n",
    "            hpo_module='random',\n",
    "            ensemble_module=None,\n",
    "            device=device, max_evals=1,\n",
    "            trainer_hp_space = fixed(\n",
    "                **{\n",
    "                    # hp from trainer\n",
    "                    \"max_epoch\": 50,\n",
    "                    \"batch_size\": 64,\n",
    "                    \"early_stopping_round\": 50 + 1,\n",
    "                    \"lr\": 0.01,\n",
    "                    \"weight_decay\": 0,\n",
    "                }\n",
    "            ),\n",
    "            model_hp_spaces=[\n",
    "                fixed(**{\n",
    "                    # hp from model\n",
    "                    \"num_layers\": 5,\n",
    "                    \"hidden\": [64,64,64,64],\n",
    "                    \"dropout\": 0.5,\n",
    "                    \"act\": \"relu\",\n",
    "                    \"eps\": \"False\",\n",
    "                    \"mlp_layers\": 2,\n",
    "                    \"neighbor_pooling_type\": \"sum\",\n",
    "                    \"graph_pooling_type\": \"sum\"\n",
    "                }) if args.model == 'gin' else fixed(**{\n",
    "                    \"ratio\": 0.8,\n",
    "                    \"dropout\": 0.5,\n",
    "                    \"act\": \"relu\"\n",
    "                }),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "# fit auto model\n",
    "solver.fit(dataset, evaluation_method=['acc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 ('tfm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bedda92429613c38dce8c018b5f564564fc8e6e3bd92ce59184d55779d13ad8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
